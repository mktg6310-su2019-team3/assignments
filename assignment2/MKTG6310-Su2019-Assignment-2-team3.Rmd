---
title: 'Segmenting for a Social Entertainment App'
subtitle: 'MKTG6310-090 Su2019 Assignment 2'
author: 'Assignment Team 3: Tianyu Xu, Sakshi Vig, Justin Peterson, Richard Mohlman, Steve Lucero, Kris Clegg'
output:
  html_notebook:
    toc: yes
  pdf_document:
    toc: no
  html_document:
    toc: no
---

```{r packages, message=FALSE, include=FALSE}
# install.packages("rmarkdown")
# install.packages("tidyverse")
# install.packages("missForest")
# install.packages("devtools")
# install.packages("caret")
# install.packages("NbClust")
# install.packages("car")
# install.packages("corrplot")
# install.packages("mclust")
# install.packages("flexmix")

library(devtools)
# devtools::install_url("http://cran.r-project.org/src/contrib/rmarkdown_1.13.tar.gz")
library(rmarkdown)
```

## R Setup 

In the course of our analysis we leveraged the following R packages and libraries:

```{r libraries, message=FALSE}
library(tidyverse)
library(missForest)
library(caret)
library(NbClust)
library(car)
library(corrplot)
library(cluster)
library(mclust)
library(flexmix)
```

## The App Happy Challenge

General Consensus at App Happy, is that there is a market for a new social entertainment app. However, App Happy currently only operates Apps in the B2B analytics category and don't yet have a product in the consumer entertainment app category.

Below we will perform a number of market segmentation analyses to explore this possible market opportunity and inform potential new product strategy and tactics including:

* descriptive post hoc segmentation analysis (a partitioning clustering method (k-means), or a hierarchical clustering method)
* predictive post hoc segmentation analysis (finite mixture regression, also called clusterwise regression, or latent class regression)

Additionally, to help App Happy better understand which respondents are using only free apps, we will build a model to predict whether new customers will only use free apps based on demographic variables.

## Part 1: Load the Data, Munge It, Do Some EDA

App Happy hired the Consumer Spy Corporation (CSC) to survey consumers in the entertainment app market. CSC collected data from a sample of consumers, and provided App Happy with a dataset of their responses. The survey questionnaire was based on preliminary qualitative research that included focus groups and one-on-one interviews.  The data collected by CSC are the base data we use to complete the below analyses.

```{r load myData, include=FALSE}
load("./appHappyData-2019.RData")
appHappyLabs <- apphappy.4.labs.frame
appHappyNums <- apphappy.4.num.frame
```

### Samples of raw Survey Data collected by CSC

```{r}
head(appHappyLabs)
head(appHappyNums)
```

```{r}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

### Recoding Likert Responses 

We opt to reverse selected numerical values so that larger numbers indicate greater agreement.

#### Question 24

```{r}
appHappyNums$q24r1 <- recode(appHappyNums$q24r1, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r2 <- recode(appHappyNums$q24r2, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r3 <- recode(appHappyNums$q24r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r4 <- recode(appHappyNums$q24r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r5 <- recode(appHappyNums$q24r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r6 <- recode(appHappyNums$q24r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r7 <- recode(appHappyNums$q24r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r8 <- recode(appHappyNums$q24r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r9 <- recode(appHappyNums$q24r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r10 <- recode(appHappyNums$q24r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r11 <- recode(appHappyNums$q24r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r12 <- recode(appHappyNums$q24r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

#### Question 25

```{r}
appHappyNums$q25r1 <- recode(appHappyNums$q25r1, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r2 <- recode(appHappyNums$q25r2, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r3 <- recode(appHappyNums$q25r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r4 <- recode(appHappyNums$q25r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r5 <- recode(appHappyNums$q25r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r6 <- recode(appHappyNums$q25r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r7 <- recode(appHappyNums$q25r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r8 <- recode(appHappyNums$q25r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r9 <- recode(appHappyNums$q25r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r10 <- recode(appHappyNums$q25r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r11 <- recode(appHappyNums$q25r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r12 <- recode(appHappyNums$q25r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

#### Question 26

```{r}
appHappyNums$q26r3 <- recode(appHappyNums$q26r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r4 <- recode(appHappyNums$q26r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r5 <- recode(appHappyNums$q26r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r6 <- recode(appHappyNums$q26r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r7 <- recode(appHappyNums$q26r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r8 <- recode(appHappyNums$q26r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r9 <- recode(appHappyNums$q26r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r10 <- recode(appHappyNums$q26r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r11 <- recode(appHappyNums$q26r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r12 <- recode(appHappyNums$q26r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r13 <- recode(appHappyNums$q26r13, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r14 <- recode(appHappyNums$q26r14, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r15 <- recode(appHappyNums$q26r15, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r16 <- recode(appHappyNums$q26r16, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r17 <- recode(appHappyNums$q26r17, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r18 <- recode(appHappyNums$q26r18, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

### Dealing with NAs

```{r}
cat('Count of NAs by column:')
naCounts <- appHappyNums %>% is.na %>% colSums
# naCounts
# cat('\n')
naCounts[naCounts>0]
```

#### Drop observations w/ NA in q12 (What % of Respondent's App Collection were Free To Download)

```{r}
appHappyNums <- appHappyNums[!is.na(appHappyNums$q12),]
```

Confirming q12 NAs have been removed

```{r, echo=False}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
# naCounts
# cat('\n')
naCounts[naCounts>0]
```

Confirm number of rows was reduced by number of NAs in q12 and number of columns matches original

```{r, echo=False}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

#### Impute values for NAs in q57 (Gender)

```{r}
set.seed(123)
appHappyNums[, -1] <- lapply(appHappyNums[, -1], factor) #casting all non-ID columns to factors for imputation
appHappyNumsImp <- appHappyNums %>% dplyr::select(-c(q5r1)) # add all but columns with NAs that we'll keep to new dataframe for imputation
appHappyNums <- appHappyNums %>% dplyr::select(c(caseID,q5r1)) # save columns w/ NAs for joining back to imputed data
appHappyNumsImp <- missForest(appHappyNumsImp)$ximp # impute missing data in q57
appHappyNums <- merge(appHappyNums, appHappyNumsImp, by="caseID") # join data back together
appHappyNums[, -1] <- lapply(appHappyNums[, -1], as.integer) #casting all non-ID columns to numeric
```

Confirm q57 NAs are gone

```{r, echo=False}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
# naCounts
# cat('\n')
naCounts[naCounts>0]
```

#### Drop Entire q5r1 Column

NAs in q5r1 are expected and are simply respondents that did not select "other" for q4

```{r}
appHappyNums <- appHappyNums %>% dplyr::select(-c(q5r1))
```

Confirm number of rows matches post-q12 NA drop and number of columns is reduced by 1

```{r, echo=False}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

Confirm no NAs remain

```{r, echo=False}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
# naCounts
# cat('\n')
naCounts[naCounts>0]
```

### Dependant Variable Creation

```{r}
appHappyNums$onlyFreeApps <- as.numeric(appHappyNums$q12==6)
unique(appHappyNums$onlyFreeApps) # confirm new binary variable generated
```

Confirm number of columns is increased by 1

```{r, echo=False}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

### Train/Test Split of the Data (80/20)

```{r}
set.seed(123)
appHappyNumsTrain <- appHappyNums %>% dplyr::sample_frac(.8)
appHappyNumsTest  <- dplyr::anti_join(appHappyNums, appHappyNumsTrain, by = 'caseID')
```

```{r, echo=False}
cat('Number of rows in train dataframe:   ',nrow(appHappyNumsTrain),'\n')
cat('Number of rows in test dataframe:     ',nrow(appHappyNumsTest),'\n')
cat('Number of rows in original dataframe:',nrow(appHappyNums))
```

## Part 2: Post Hoc Descriptive Segmentation Analysis and Profiling  

App Happy wants to segment the market based on customers' _attitudes_. Questionnaire items q24, q25, and q26 measure various attitudes. We will begin by creating new datasets to evaluate these questionnaire items.   

```{r}
#Create new datasets to evaluate Q24, Q25, and Q26
appHappyNu_q24 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12')]
appHappyNu_q25 <- appHappyNums[,c('q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12')]
appHappyNu_q26 <- appHappyNums[,c('q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
appHappyNu_q24_q25 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12','q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12')]
appHappyNu_q24_q26 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12','q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
appHappyNu_q25_q26 <- appHappyNums[,c('q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12','q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
```

### Distribution Analysis

Create boxplots to find which questions have high variation/low variation, as well as see differences in average response by question type. Also creat the correlation plots comparing each question. 

```{r}
boxplot(appHappyNu_q24, col=rainbow(length(unique(appHappyNu_q24))))
boxplot(appHappyNu_q25, col=rainbow(length(unique(appHappyNu_q25))))
boxplot(appHappyNu_q26, col=rainbow(length(unique(appHappyNu_q26))))
```

#### Boxplot Observations

* Evalutaing the boxplots we can see which questions have very little variance compared to the rest of the questions. 
* Questions that jump out immediately with little variance are q24r2, q24r6, q25r8, q25r11, q26r9, q26r15, and q26r17. These responses were all almost exlusively in agreement to the questions. 
* From the boxplots we can also see that the majority of questions people agreed with. There are a few that people generally did not agree with, like q24r9, q25r6, q26r11. The rest of the responses seemed to have a pretty good spread of responses across all possibilities. 

### Correlation Analysis

Next we will evaluate the correlation plots to evaluate correlationsions of responses from the same question, and comparing groups of questions. 

First, we evaluate correlation between sets of responses within each question:

```{r}
M1 <- cor(appHappyNu_q24) # get correlations
M2 <- cor(appHappyNu_q25) # get correlations
M3 <- cor(appHappyNu_q26) # get correlations
```

```{r}
corrplot(M1, method = "circle") # plot matrix
corrplot(M2, method = "circle") # plot matrix
corrplot(M3, method = "circle") # plot matrix
```

Next, we evaluate correlations across question sets:

```{r}
M4 <- cor(appHappyNu_q24_q25) # get correlations
M5 <- cor(appHappyNu_q24_q26) # get correlations
M6 <- cor(appHappyNu_q25_q26) # get correlations
```

```{r}
corrplot(M4, method = "circle") # plot matrix
corrplot(M5, method = "circle") # plot matrix
corrplot(M6, method = "circle") # plot matrix
```

#### Observations

* Within Q24, r9 and r4 were most strongly positively correlated with one another, while most other items generally had smaller, yet still positive correlations with one another. r12 seemed to have the most possitive correlations with several other questions. 
* Within Q25, all items except r6 and r12 were strongly positively correlated with one another.
* Within Q26, all items were positively correlated with one another, with r7 and r18 most strongly positively correlated with one another.
* Correlations appear to be stronger when comparing responses within questions as opposed to correlation across question sets q24, q25 & q26.

### K-means Cluster Analysis

Next we are going to perform a k-means clustering analysis. First we need to find out the ideal number of clusters to use for the K-Means Cluster. We decided to use two methods, the **elbow method** and **NBClust** to determine what the optimal number of clusters are. 

#### Elbow Method

The 'elbow method' can be used to determine the optimal number of clusters. We want to look for the **elbow** or **knee** of the graph (where the curve flattens out).

```{r, warning=F}
wss <- function(k){
                  kmeans(appHappyNu_q24,k,nstart=10)$tot.withinss
                  }

kRange <- 1:12
wssValues <- unlist(lapply(kRange, wss))

# plot where x-axis is kRange and y-axis is wssValues
plot(x=kRange,y=wssValues)
```
* Elbow appears to be between **2** and **3**

#### NBClust Method

```{r, warning=FALSE}
# K Means clustering method using NBClust
numClusts <- NbClust(appHappyNu_q24, 
                     min.nc = 2, 
                     max.nc = 12, 
                     method = "kmeans", 
                     index = "gap")
numClusts$Best.nc
```

* NBClust method recommended using 2 clusters. 

> We decided to proceed and use 3 clusters. 

### Create Clusters

```{r, warning=F}
# Set the seed to be able to have others get the same results as us using a random process
set.seed(5)
clusters <- kmeans(appHappyNu_q24,
                   3,
                   nstart=15)
clusters$size # number of records assigned to each cluster
```


### Cluster Evaluation

#### Visualization
First we graph out the clusters to get a visual representation. 

```{r, warning=F}
clusplot(appHappyNu_q24,clus=clusters$cluster,col.p=clusters$cluster)
```

#### Average responses for each cluster. 

```{r, warning=F}
#let us find the cluster centers
clusters$centers
```

#### Cluster Demographics 

Last we'll take a look at each cluster to see if any cluster has an unequal amount of gender, race, income, etc. 

##### Education: Question 48 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q48,clusters$cluster)
```

##### Marital Status: Question 49 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q49,clusters$cluster)
```

##### Race: Question 54 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q54,clusters$cluster)
```

##### Hispanic: Question 55 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q55,clusters$cluster)
```

##### Household Income: Question 56 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q56,clusters$cluster)
```

##### Gender: Question 57 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q57,clusters$cluster)
```

##### Our Observations:

* The disproportions that jump out are cluster #2 has an unequal proportion of White respondants, with very few Asian respondents. Cluster #2 seems to be the least diverse cluster with the fewest Hispanic respondents despite being the largest cluster in terms of size. 
* Cluster #1 has about half the respondants of people making $150K or more than the other two groups do. 
* Cluster #2 appears to be heavily weighted female, while the other two clusters are more balanced. 

### Profiling

Looking at the cluster features helps us distinguish the different viewpoints of each cluster/group. Comparing how each cluster responded on allows us to profile each cluster to see their differences. Here are the profiles of each cluster:

* Cluster 1 - **Tech Savvy Group**, focus on important technological developments. They feel that there's too much technology or information today, focus on just the important technology. Best to market them then new and important technologies. 
* Cluster 2 - **Tech Users Group**, doesn't keep up on important technological developments as much as cluster 1. They use technology in all forms and want all the technology that can be produced. Best to market older revamped technologies. 
* Cluster 3 - **Not Tech Savvy Group**, this cluster feels less confident about using technology and it is not as big of a part of their life as cluster 2 and 3. Probably not the best group to market new technologies to, but rather the easily adoptable technologies. 

### Conclusion: 

This cluster analysis allows us to target specific users for the new app. Specifically by coming up with a profile, we learn what is important to each cluster/group. 

For the purposes of marketing this new app, **App Happy should likely focus its efforts on cluster #2**, as those people will buy in quickly and use it often. However, they will need to convince those people why they need to keep using their app instead of adopting other technologies. 

As a secondary market strategy, they can also **target cluster #1** with a different focus on why this new app is different and ground breaking. Try convincing this group why this tech is different and they will buy in if the argument is sound enough. 

## Part 3: Probability of Using Only Free Apps

In order to estimate whether a respondent **only** uses free apps, we created a new binary dependent variable called `onlyFreeApps` that has a value of 1 if the value of variable q12 equals 6, and is 0 if q12 equals 1,2,3,4 or 5.

During EDA we randomly split the data into a model estimation training sample and a model test sample (80% training & 20% test).

We use the demographic variables in the survey data (q1, q48-q53, q55-q57) as predictor variables in both a Probit and Logit Regession Model. We omit variable q54 because attitude towards the use of Free Apps is subjective and it does not show any pattern with the race of the customer, therefore, q54 does not have significance in a model. There is also legal concerns with developing marketing strategies based off race.

### Binary Logit Regression Model

```{r}
#Creating the binary logistic regression Model (Taking respondent with only free apps as a function of q1, q48 and q49 (Independent Variables))
logitModel <- glm(onlyFreeApps ~ q1+q48+q49,
                 data=appHappyNumsTrain,
                 family=binomial(link=logit)) 
summary(logitModel)
```

```{r}
#Predict Model using logit Model
lModelPreds <- predict(logitModel,
                      newdata=appHappyNumsTest,
                      type="response") 
summary(lModelPreds)
```

```{r}
# Predictive Accuracy of the Logit Model
lPreds <- as.numeric(lModelPreds>=0.5) 
table(lPreds) 

cat('\n')

lActuals <- appHappyNumsTest['onlyFreeApps'] 
table(actuals) 

cat('\nLogit Model Accuracy:',100*sum(lPreds==lActuals,na.rm=TRUE)/length(lPreds))
```

### Binary Probit Regression Model

```{r}
#Creating binary Probit Regression Model(Taking respondent with only free apps as a function of q1, q48 and q49 (Independent Variables))
probitModel=glm(onlyFreeApps~q1+q48+q49,
                data=appHappyNumsTrain,
                family=binomial(link=probit)) 
summary(probitModel)
```

```{r}
#Predict Model using probitModel
pModelPreds=predict(probitModel,
                   newdata=appHappyNumsTest,
                   type="response") 
summary(pModelPreds)
```

```{r}
#Predictive accuracy of the Model
pPreds <- as.numeric(pModelPreds>=0.5) 
table(pPreds)

cat('\n')

pActuals=appHappyNumsTest['onlyFreeApps'] 
table(pActuals)

cat('\nProbit Model Accuracy:',100*sum(pPreds==pActuals,na.rm=TRUE)/length(pPreds))
```

#### Difference between predictive accuracy perecentage of both models

The predictive accuracy percentage for both the models is 78.90%.

The logit and probit models are critical parts of analysing data. We generally use to know that if a covariate has the same effect for different groups, Unfortunately, inorder to compare the effect of covariates across groups, we make an assumption that each group has the same residual variation. In this case we can compare the coefficients to determine which model is better fit but in this case it reveals that there is no significant difference in coefficients hence, no difference between the models.

Respondents only using free apps are taken as a dependent variable is a function of q1, q48 and q49 as independent variables. This means the relation depends on: 
1. Age 
2. Marital status 
3. Education. 

For Logit and Probit Model give us the following inferences:
* As the age increases Usage of free apps will increase.
* As the education level increases from High school to Post Graduate usage of free app decreases

>Predictive accuracy in terms of percent correct prediction of responses, for both the **estimation** data and for the **test** data.  (Question: what do differences between the two tell you about your model?).
>Your "best" model should only include: **predictors that are statistically significant** at the conventional level, **a Type I error rate of 0.05**, and whose **accuracy doesn't decrease much (or at all) when it is used to predict `onlyFreeApps` using the held out, 20% test data**.
>Interpret your best model's results for App Happy.  Mention any assumptions that you think might be important to for Appy Happy to consider when interpreting your model's results.

## Part 4: Post Hoc Predictive Segmentation Analysis

```{r input-assignment-data}
appNum <- appHappyNums
class(appNum)
dim(appNum)
```

Because we want to know how their attitudes predict whether they pay for apps or not, we select column: q24, q25, q26 and q12:

```{r select-vars}
appNumS <- appNum %>% select(q24r1:q24r12,q25r1:q25r12,q26r3:q26r17,q26r18,q12,onlyFreeApps)
dim(appNumS)
```

### Finite Mixture Binary Logit: Search for "best" clustering

Let's search for a solution that has from 1 to 6 clusters. Note how `onlyFreeApps` is expressed as the dependent variable in the model formula: `cbind(onlyFreeApps, 1 - onlyFreeApps)`.

```{r binlog-search}
binLogSearch=stepFlexmix(cbind(onlyFreeApps, 1 - onlyFreeApps) ~ 1, data = appNumS, 
model = FLXMRglmfix(family = "binomial", 
fixed=~q24r1 + q24r2 + q24r3 + q24r4 + q24r5 + q24r6 + q24r7 + q24r8 + q24r9 + q24r10 + q24r11 + q24r12 + q25r1 + q25r2 + q25r3 + q25r4 + q25r5 + q25r6 + q25r7 + q25r8 + q25r9 + q25r10 + q25r11 + q25r12 + q26r3 + q26r4 + q26r5 + q26r6 + q26r7 + q26r8 + q26r9 + q26r10 + q26r11 + q26r12 + q26r13 + q26r14 + q26r15 + q26r16 + q26r17 + q26r18),
k = 1:6, nrep = 3)
```

Now we can see which cluster would best to fit data. As we can see below, the 2 clustering has highest loglikelihood, so we choose 2 clustering to make the segements.

```{r}
binLogSearch
```

Now we focus on the two-components model's results:

```{r two-comp-binLog}
binLogModk1=stepFlexmix(cbind(onlyFreeApps, 1 - onlyFreeApps) ~ 1, data = appNumS, 
model = FLXMRglmfix(family = "binomial", 
fixed=~q24r1 + q24r2 + q24r3 + q24r4 + q24r5 + q24r6 + q24r7 + q24r8 + q24r9 + q24r10 + q24r11 + q24r12 + q25r1 + q25r2 + q25r3 + q25r4 + q25r5 + q25r6 + q25r7 + q25r8 + q25r9 + q25r10 + q25r11 + q25r12 + q26r3 + q26r18),
k = 2, nrep = 3)
```

```{r binLogModk1-summary, warning=False}
summary(refit(binLogModk1))
```

```{r}
cat('\nAIC:', AIC(binLogModk1))
cat('\nlog likelihood:',logLik(binLogModk1))
```

According to the results below, one of the segments has 50.01% prior probability and another segement has total 49.99% prior probability. They also have -726.3169 log likelihood with AIC 1718.634. One of the difference for both segements is the size, one segment has 1457 objects while another has only 186 objects. The ratio for first clustering close to 1 means that the first clustering more tight than the another one.
```{r}
summary(binLogModk1)
```

>App Happy wants to know if potential customers differ in terms of how their attitudes predict the likelihood of paying for apps. The question is, are there customer segments that differ in terms of how their attitudes predict whether they pay for apps or not?
>Estimate binary logit finite mixture models to predict 1 minus `onlyFreeApps` to identify the one that best describes the data. Profile these segments, and describe how each segment uniquely differs from the other segments, and what these differences suggest for App Happy's plan to market it's app.

### Reproducible Marketing Data Science Assignment Work Product  

>Assemble your work in an R Notebook.  Organize your Notebook so that it effectively shows and explains what you did, and what you found, to other persons who might want to replicate your work.  It should exemplify good reproducible data science practice.  Don't exceed **16** (_16_) pages in length in your pdf document.  Staying within this limit may require you to decide what's important to include in your report, and what can be gotten rid of. A design principle is to "keep what's essential, and get rid of the rest."  Include what's important for a reader to understand your story.  Don't be WET ('waste everyone's time') by including content that's unimportant or irrelavant.
