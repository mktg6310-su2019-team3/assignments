---
title: "Segmenting for a Social Entertainment App"
author: "Assignment Team 3: Tianyu Xu, Sakshi Vig, Justin Peterson, Richard Mohlman, Steve Lucero, Kris Clegg"
subtitle: 'MKTG6310-090 Su2019 Assignment 2'
output:
  word_document:
    toc: no
  html_notebook:
    toc: yes
  pdf_document:
    toc: no
  html_document:
    toc: no
---

```{r packages, message=FALSE, include=FALSE}
# install.packages("rmarkdown")
# install.packages("tidyverse")
# install.packages("missForest")
# install.packages("devtools")
# install.packages("caret")
# install.packages("NbClust")
# install.packages("car")
# install.packages("corrplot")
# install.packages("mclust")
# install.packages("flexmix")
# install.packages("InformationValue")

library(devtools)
# devtools::install_url("http://cran.r-project.org/src/contrib/rmarkdown_1.13.tar.gz")
library(rmarkdown)

knitr::opts_chunk$set(fig.width=6, fig.height=4)
```

## R Setup 

In the course of our analysis we leveraged the following R packages and libraries:

```{r libraries, message=FALSE}
library(tidyverse)
library(missForest)
library(caret)
library(NbClust)
library(car)
library(corrplot)
library(cluster)
library(mclust)
library(flexmix)
library(InformationValue)
```

## The App Happy Challenge

General Consensus at App Happy, is that there is a market for a new social entertainment app. However, App Happy currently only operates Apps in the B2B analytics category and don't yet have a product in the consumer entertainment app category.

Below we will perform a number of market segmentation analyses to explore this possible market opportunity and inform potential new product strategy and tactics including:

* descriptive post hoc segmentation analysis (a partitioning clustering method (k-means), or a hierarchical clustering method)
* predictive post hoc segmentation analysis (finite mixture regression, also called clusterwise regression, or latent class regression)

Additionally, to help App Happy better understand which respondents are using only free apps, we will build a model to predict whether new customers will only use free apps based on demographic variables.

## Part 1: Exploratory Data Analysis

App Happy hired the Consumer Spy Corporation (CSC) to survey consumers in the entertainment app market. CSC collected data from a sample of consumers, and provided App Happy with a dataset of their responses. The survey questionnaire was based on preliminary qualitative research that included focus groups and one-on-one interviews.  The data collected by CSC are the base data we use to complete the below analyses.

```{r load myData, include=FALSE}
load("./appHappyData-2019.RData")
appHappyLabs <- apphappy.4.labs.frame
appHappyNums <- apphappy.4.num.frame
```

### Recoding Likert Responses 

We opt to reverse selected numerical values so that larger numbers indicate greater agreement.

#### Question 24

```{r}
appHappyNums$q24r1 <- recode(appHappyNums$q24r1, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r2 <- recode(appHappyNums$q24r2, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r3 <- recode(appHappyNums$q24r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r4 <- recode(appHappyNums$q24r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r5 <- recode(appHappyNums$q24r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r6 <- recode(appHappyNums$q24r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r7 <- recode(appHappyNums$q24r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r8 <- recode(appHappyNums$q24r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r9 <- recode(appHappyNums$q24r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r10 <- recode(appHappyNums$q24r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r11 <- recode(appHappyNums$q24r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q24r12 <- recode(appHappyNums$q24r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

#### Question 25

```{r}
appHappyNums$q25r1 <- recode(appHappyNums$q25r1, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r2 <- recode(appHappyNums$q25r2, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r3 <- recode(appHappyNums$q25r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r4 <- recode(appHappyNums$q25r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r5 <- recode(appHappyNums$q25r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r6 <- recode(appHappyNums$q25r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r7 <- recode(appHappyNums$q25r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r8 <- recode(appHappyNums$q25r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r9 <- recode(appHappyNums$q25r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r10 <- recode(appHappyNums$q25r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r11 <- recode(appHappyNums$q25r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q25r12 <- recode(appHappyNums$q25r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

#### Question 26

```{r}
appHappyNums$q26r3 <- recode(appHappyNums$q26r3, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r4 <- recode(appHappyNums$q26r4, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r5 <- recode(appHappyNums$q26r5, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r6 <- recode(appHappyNums$q26r6, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r7 <- recode(appHappyNums$q26r7, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r8 <- recode(appHappyNums$q26r8, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r9 <- recode(appHappyNums$q26r9, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r10 <- recode(appHappyNums$q26r10, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r11 <- recode(appHappyNums$q26r11, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r12 <- recode(appHappyNums$q26r12, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r13 <- recode(appHappyNums$q26r13, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r14 <- recode(appHappyNums$q26r14, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r15 <- recode(appHappyNums$q26r15, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r16 <- recode(appHappyNums$q26r16, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r17 <- recode(appHappyNums$q26r17, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
appHappyNums$q26r18 <- recode(appHappyNums$q26r18, "1=6; 2=5; 3=4; 4=3; 5=2; 6=1")
```

### Dealing with NAs

Calculating the number of records/columns prior to cleaning NAs.

```{r}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

Determining how many NAs exist.

```{r}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
naCounts[naCounts>0]
```

#### Drop observations w/ NA in q12 (What % of Respondent's App Collection were Free To Download)

```{r}
appHappyNums <- appHappyNums[!is.na(appHappyNums$q12),]
```

Confirming q12 NAs have been removed:

```{r}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
naCounts[naCounts>0]
```

Confirm number of rows was reduced by number of NAs in q12 and number of columns matches original.

```{r}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

#### Impute values for NAs in q57 (Gender)

```{r}
set.seed(123)
appHappyNums[, -1] <- lapply(appHappyNums[, -1], factor) #casting all non-ID columns to factors for imputation
appHappyNumsImp <- appHappyNums %>% dplyr::select(-c(q5r1)) # add all but columns with NAs that we'll keep to new dataframe for imputation
appHappyNums <- appHappyNums %>% dplyr::select(c(caseID,q5r1)) # save columns w/ NAs for joining back to imputed data
appHappyNumsImp <- missForest(appHappyNumsImp)$ximp # impute missing data in q57
appHappyNums <- merge(appHappyNums, appHappyNumsImp, by="caseID") # join data back together
appHappyNums[, -1] <- lapply(appHappyNums[, -1], as.integer) #casting all non-ID columns back to numeric (this time integers)
```

Confirm q57 NAs are gone

```{r}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
naCounts[naCounts>0]
```

#### Drop Entire q5r1 Column

NAs in q5r1 are expected and are simply respondents that did not select "other" for q4.

```{r}
appHappyNums <- appHappyNums %>% dplyr::select(-c(q5r1))
```

Confirm number of rows matches post-q12 NA drop and number of columns is reduced by 1

```{r}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

Confirm no NAs remain

```{r}
cat('Count of NAs by column:\n')
naCounts <- appHappyNums %>% is.na %>% colSums
naCounts[naCounts>0]
```

### Dependant Variable Creation

Creating a new variable to use to predict whether individuals are likely to only use free apps.

```{r}
appHappyNums$onlyFreeApps <- as.numeric(appHappyNums$q12==6)
table(appHappyNums$q12, appHappyNums$onlyFreeApps) # confirm new binary variable generated
```

Confirm number of columns is increased by 1

```{r}
cat('Number of records (rows, observations):', nrow(appHappyNums), '\n')
cat('Number of variables (columns):', ncol(appHappyNums), '\n')
```

### Train/Test Split of the Data (80/20)

Splitting the data for onlyFreeApps model training and testing.

```{r}
set.seed(123)
appHappyNumsTrain <- appHappyNums %>% dplyr::sample_frac(.8)
appHappyNumsTest  <- dplyr::anti_join(appHappyNums, appHappyNumsTrain, by = 'caseID')
```

```{r}
cat('Number of rows in train dataframe:   ',nrow(appHappyNumsTrain),'\n')
cat('Number of rows in test dataframe:     ',nrow(appHappyNumsTest),'\n')
cat('Number of rows in original dataframe:',nrow(appHappyNums))
```

## Part 2: Post Hoc Descriptive Segmentation Analysis and Profiling  

App Happy wants to segment the market based on customers' _attitudes_. Questionnaire items q24, q25, and q26 measure various attitudes. We will begin by creating new datasets to evaluate these questionnaire items.   

```{r}
#Create new datasets to evaluate Q24, Q25, and Q26
appHappyNu_q24 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12')]
appHappyNu_q25 <- appHappyNums[,c('q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12')]
appHappyNu_q26 <- appHappyNums[,c('q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
appHappyNu_q24_q25 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12','q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12')]
appHappyNu_q24_q26 <- appHappyNums[,c('q24r1','q24r2','q24r3','q24r4','q24r5','q24r6','q24r7','q24r8','q24r9','q24r10','q24r11','q24r12','q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
appHappyNu_q25_q26 <- appHappyNums[,c('q25r1','q25r2','q25r3','q25r4','q25r5','q25r6','q25r7','q25r8','q25r9','q25r10','q25r11','q25r12','q26r3','q26r4','q26r5','q26r6','q26r7','q26r8','q26r9','q26r10','q26r11','q26r12','q26r13','q26r14','q26r15','q26r16','q26r17','q26r18')]
```

### Distribution Analysis

Create boxplots to find which questions have high variation/low variation, as well as see differences in average response by question type. Also create the correlation plots comparing each question. We	have a	plot	for	q24	to	provide	a	different	view	of	our	distribution	analysis.

```{r, out.width = '60%'}
boxplot(appHappyNu_q24, col=rainbow(length(unique(appHappyNu_q24))))
```

#### Boxplot Observations

* Evalutaing the boxplots we can see which questions have very little variance compared to the rest of the questions. 
* Questions that jump out immediately with little variance are q24r2, q24r6, q25r8, q25r11, q26r9, q26r15, and q26r17. These responses were all almost exlusively in agreement to the questions. 
* From the boxplots we can also see that the majority of questions people agreed with. There are a few that people generally did not agree with, like q24r9, q25r6, q26r11. The rest of the responses seemed to have a pretty good spread of responses across all possibilities. 

### Correlation Analysis

Next we will evaluate the correlation plots to evaluate correlationsions of responses from the same question, and comparing groups of questions. 

First, we evaluate correlation between sets of responses within each question:

```{r}
M1 <- cor(appHappyNu_q24) # get correlations
M2 <- cor(appHappyNu_q25) # get correlations
M3 <- cor(appHappyNu_q26) # get correlations
```

```{r, out.width = '50%'}
corrplot(M1, method = "circle") # plot matrix
corrplot(M2, method = "circle") # plot matrix
corrplot(M3, method = "circle") # plot matrix
```

We also took a look at the between question correlation matrices to see if any correlation patterns emerged there. We did not find anything significant and found that the between response correlations were stronger. As a result, we did not include those correlation matrices.

#### Observations

* Within Q24, r9 and r4 were most strongly positively correlated with one another, while most other items generally had smaller, yet still positive correlations with one another. r12 seemed to have the most possitive correlations with several other questions. 
* Within Q25, all items except r6 and r12 were strongly positively correlated with one another.
* Within Q26, all items were positively correlated with one another, with r7 and r18 most strongly positively correlated with one another.
* Correlations appear to be stronger when comparing responses within questions as opposed to correlation across question sets q24, q25 & q26.

### K-means Cluster Analysis

Next we are going to perform a k-means clustering analysis. First we need to find out the ideal number of clusters to use for the K-Means Cluster. We decided to use two methods, the **elbow method** and **NBClust** to determine what the optimal number of clusters are. 

#### Elbow Method

The 'elbow method' can be used to determine the optimal number of clusters. We want to look for the **elbow** or **knee** of the graph (where the curve flattens out).

```{r, warning=F}
wss <- function(k){
                  kmeans(appHappyNu_q24,k,nstart=10)$tot.withinss
                  }

kRange <- 1:12
wssValues <- unlist(lapply(kRange, wss))

# plot where x-axis is kRange and y-axis is wssValues
plot(x=kRange,y=wssValues)
```
* Elbow appears to be between **2** and **3**

#### NBClust Method

```{r, warning=FALSE}
# K Means clustering method using NBClust
numClusts <- NbClust(appHappyNu_q24, 
                     min.nc = 2, 
                     max.nc = 12, 
                     method = "kmeans", 
                     index = "gap")
numClusts$Best.nc
```

* NBClust method recommended using 2 clusters. 

> We decided to proceed and use 3 clusters. 

### Create Clusters

```{r, warning=F}
# Set the seed to be able to have others get the same results as us using a random process
set.seed(5)
clusters <- kmeans(appHappyNu_q24,
                   3,
                   nstart=15)
clusters$size # number of records assigned to each cluster
```


### Cluster Evaluation

#### Visualization
First we graph out the clusters to get a visual representation. 

```{r, warning=F}
clusplot(appHappyNu_q24,clus=clusters$cluster,col.p=clusters$cluster)
```

#### Average responses for each cluster. 

```{r, warning=F}
#let us find the cluster centers
clusters$centers
```

#### Cluster Demographics 

Last we'll take a look at each cluster to see if any cluster has an unequal amount of gender, race, income, etc. 

##### Education: Question 48 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q48,clusters$cluster)
```

##### Marital Status: Question 49 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q49,clusters$cluster)
```

##### Race: Question 54 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q54,clusters$cluster)
```

##### Hispanic: Question 55 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q55,clusters$cluster)
```

##### Household Income: Question 56 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q56,clusters$cluster)
```

##### Gender: Question 57 Response Distribution by Cluster
```{r, warning=F}
table(appHappyNums$q57,clusters$cluster)
```

##### Our Observations:

* The disproportions that jump out are cluster #2 has an unequal proportion of White respondants, with very few Asian respondents. Cluster #2 seems to be the least diverse cluster with the fewest Hispanic respondents despite being the largest cluster in terms of size. 
* Cluster #1 has about half the respondants of people making $150K or more than the other two groups do. 
* Cluster #2 appears to be heavily weighted female, while the other two clusters are more balanced. 

### Profiling

Looking at the cluster features helps us distinguish the different viewpoints of each cluster/group. Comparing how each cluster responded on allows us to profile each cluster to see their differences. Here are the profiles of each cluster:

* Cluster 1 - **Tech Savvy Group**, focus on important technological developments. They feel that there's too much technology or information today, focus on just the important technology. Best to market them then new and important technologies. 
* Cluster 2 - **Tech Users Group**, doesn't keep up on important technological developments as much as cluster 1. They use technology in all forms and want all the technology that can be produced. Best to market older revamped technologies. 
* Cluster 3 - **Not Tech Savvy Group**, this cluster feels less confident about using technology and it is not as big of a part of their life as cluster 2 and 3. Probably not the best group to market new technologies to, but rather the easily adoptable technologies. 

### Conclusion: 

This cluster analysis allows us to target specific users for the new app. Specifically by coming up with a profile, we learn what is important to each cluster/group. 

For the purposes of marketing this new app, **App Happy should likely focus its efforts on cluster #2**, as those people will buy in quickly and use it often. However, they will need to convince those people why they need to keep using their app instead of adopting other technologies. 

As a secondary market strategy, they can also **target cluster #1** with a different focus on why this new app is different and ground breaking. Try convincing this group why this tech is different and they will buy in if the argument is sound enough. 

## Part 3: Probability of Using Only Free Apps

In order to estimate whether a respondent **only** uses free apps, we created a new binary dependent variable called `onlyFreeApps` that has a value of 1 if the value of variable q12 equals 6, and is 0 if q12 equals 1,2,3,4 or 5.

During EDA we randomly split the data into a model estimation training sample and a model test sample (80% training & 20% test).

We use the demographic variables in the survey data (q1, q48-q53, q55-q57) as predictor variables in both a Probit and Logit Regession Model. We omit variable q54 because attitude towards the use of Free Apps is subjective and it does not show any pattern with the race of the customer, therefore, q54 does not have significance in a model. There is also legal concerns with developing marketing strategies based off race.

### Binary Logit Regression Model

Building a Binary Logit Regression Model to predict 'onlyFreeApps' value based on demographic data.

#### Predictor Selection

```{r}
logitModel <- glm(onlyFreeApps ~ q1+q48+q49+q50r1+q50r2+q50r3+q50r4+q50r5+q55+q56+q57,
                 data=appHappyNumsTrain,
                 family=binomial(link=logit))
summary(logitModel)
```

```{r}
logitModel <- glm(onlyFreeApps ~ q48+q55+q56+q57,
                 data=appHappyNumsTrain,
                 family=binomial(link=logit))
summary(logitModel)
```

Standard p-value threshold for statistical significance is .05, therefore, we will build our model using only predictors that meet that threshold: q48, q55, q56 & q57.

Additionally, the smaller the AIC value, the better the fit of a model and general consensus is that movement in AIC of 5+ points is meaningful. We are able to reduce the AIC for a model with all predictors from 1330.8 to 1319.9 in a model with only 4 predictors.

#### Predicting on the Training Data Set

For a binary logistic regression, we need to pay close attention to the cutoff value used against the resulting probability output. We leverage the `InformationValue` package to select the optimal cutoff value.


```{r}
logitModelTrainActuals <- appHappyNumsTrain$onlyFreeApps
logitModelTrainPreds <- predict(logitModel, newdata=appHappyNumsTrain, type="response")
optCut <- optimalCutoff(logitModelTrainActuals, logitModelTrainPreds, optimiseFor = "misclasserror")
logitModelTrainPreds <- as.numeric(logitModelTrainPreds>=optCut)
cat('Optimal Cutoff Value:',optCut)
```

Predictive Accuracy of the Logit Model on the Training Data:
```{r}
table(logitModelTrainPreds)
cat('\n')
table(logitModelTrainActuals)
cat('\nLogit Model Accuracy:',100*sum(logitModelTrainPreds==logitModelTrainActuals,na.rm=TRUE)/length(logitModelTrainPreds))
cat('\n')
confusionMatrix(logitModelTrainActuals,logitModelTrainPreds)
cat('\nType I Error Rate:',confusionMatrix(logitModelTrainActuals,logitModelTrainPreds)[2,1]/length(logitModelTrainActuals))
```

#### Predicting on the Test Data Set

```{r}
logitModelTestActuals <- appHappyNumsTest$onlyFreeApps
logitModelTestPreds <- predict(logitModel, newdata=appHappyNumsTest, type="response")
logitModelTestPreds <- as.numeric(logitModelTestPreds>=optCut)
cat('Optimal Cutoff Value:',optCut)
```

Predictive Accuracy of the Logit Model on the Test Data:
```{r}
table(logitModelTestPreds)
cat('\n')
table(logitModelTestActuals)
cat('\nLogit Model Accuracy:',100*sum(logitModelTestPreds==logitModelTestActuals,na.rm=TRUE)/length(logitModelTestPreds))
cat('\n')
confusionMatrix(logitModelTestActuals,logitModelTestPreds)
cat('\nType I Error Rate:',confusionMatrix(logitModelTestActuals,logitModelTestPreds)[2,1]/length(logitModelTestActuals))
```

#### Difference between predictive accuracy perecentage of both models

The predictive accuracy percentage for both the training and test data sets is 78.4%. The model appears to have the same predictive accuracy on any data whether its the data the model was trained on or out-of-sample data for the same survey. We also note that the Type I Error rate on both Training and Test data sets is at or near 0%.

Respondents only using free apps are taken as a dependent variable is a function of q48, q55, q56 & q57 as independent variables. This means the relation depends on:

1. (q48) Marital status
2. (q55) Hispanic Ethnicity
3. (q56) Household Income
4. (q57) Gender

Our Logit Model give us the following inferences:

* Married respondents are less likely to spend on apps
* Hispanics are less likely to spend on apps
* As household income increases, respondents are more likely to spend on apps
* Females are more likely to spend on apps

Important to note that this model was created simply to determine factors that drive spend on apps, not necessarily the quantity of spend (i.e. the industry term known as 'whales' aka high-spenders).

## Part 4: Post Hoc Predictive Segmentation Analysis

```{r input-assignment-data}
appNum <- appHappyNums
dim(appNum)
```

Because we want to know how their attitudes predict whether they pay for apps or not, we select column: q24, q25, q26 and q12:

```{r select-vars}
appNumS <- appNum %>% select(q24r1:q24r12,q25r1:q25r12,q26r3:q26r17,q26r18,q12,onlyFreeApps)
dim(appNumS)
```

### Finite Mixture Binary Logit: Search for "best" clustering

Let's search for a solution that has from 1 to 6 clusters. Note how `onlyFreeApps` is expressed as the dependent variable in the model formula: `cbind(onlyFreeApps, 1 - onlyFreeApps)`.

```{r binlog-search}
binLogSearch=stepFlexmix(cbind(onlyFreeApps, 1 - onlyFreeApps) ~ 1, data = appNumS, 
model = FLXMRglmfix(family = "binomial", 
fixed=~q24r1 + q24r2 + q24r3 + q24r4 + q24r5 + q24r6 + q24r7 + q24r8 + q24r9 + q24r10 + q24r11 + q24r12 + q25r1 + q25r2 + q25r3 + q25r4 + q25r5 + q25r6 + q25r7 + q25r8 + q25r9 + q25r10 + q25r11 + q25r12 + q26r3 + q26r4 + q26r5 + q26r6 + q26r7 + q26r8 + q26r9 + q26r10 + q26r11 + q26r12 + q26r13 + q26r14 + q26r15 + q26r16 + q26r17 + q26r18),
k = 1:6, nrep = 3)
```

Now we can see which cluster would best to fit data. As we can see below, the 2 clustering has highest loglikelihood, so we choose 2 clustering to make the segements.

```{r}
binLogSearch
```

Now we focus on the two-components model's results:

```{r two-comp-binLog}
binLogModk1=stepFlexmix(cbind(onlyFreeApps, 1 - onlyFreeApps) ~ 1, data = appNumS, 
model = FLXMRglmfix(family = "binomial", 
fixed=~q24r1 + q24r2 + q24r3 + q24r4 + q24r5 + q24r6 + q24r7 + q24r8 + q24r9 + q24r10 + q24r11 + q24r12 + q25r1 + q25r2 + q25r3 + q25r4 + q25r5 + q25r6 + q25r7 + q25r8 + q25r9 + q25r10 + q25r11 + q25r12 + q26r3 + q26r18),
k = 2, nrep = 3)
```

```{r binLogModk1-summary}
summary(refit(binLogModk1))
```

```{r}
cat('\nAIC:', AIC(binLogModk1))
cat('\nlog likelihood:',logLik(binLogModk1))
```

According to the results below, one of the segments has 50.01% prior probability and another segement has total 49.99% prior probability. They also have -726.3169 log likelihood with AIC 1718.634. One of the difference for both segements is the size, one segment has 1457 objects while another has only 186 objects. The ratio for first clustering close to 1 means that the first clustering more tight than the another one.
```{r}
summary(binLogModk1)
```

### Conclusions

In conclusion. The optimum number of segments the mclust model produces is 2. This is consistent with the flexmix model, so we presume that the accuracy is high. The likelihood and BIC are very low, so the likelihood that this model is close to the truth is high. Because of this we are unable to calculate any differences between segments since all observations are in the same segment.
